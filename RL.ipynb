{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s implement a basic reinforcement learning (RL) example using Q-learning, a simple but powerful RL algorithm.\n",
    "\n",
    "Weâ€™ll use a small grid environment to demonstrate the idea. Here's a basic Python implementation without any external libraries (except numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''ðŸ§  Q-Learning in a GridWorld\n",
    "ðŸ—º Environment Description:\n",
    "4x4 Grid.\n",
    "\n",
    "Agent starts at (0, 0).\n",
    "\n",
    "Goal at (3, 3) â†’ +10 reward.\n",
    "\n",
    "Hitting walls â†’ stay in place.\n",
    "\n",
    "Each move â†’ -1 reward.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment size\n",
    "grid_size = 4\n",
    "\n",
    "# Actions: up, down, left, right\n",
    "actions = ['U', 'D', 'L', 'R']\n",
    "action_dict = {\n",
    "    'U': (-1, 0),\n",
    "    'D': (1, 0),\n",
    "    'L': (0, -1),\n",
    "    'R': (0, 1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Q-table initialization\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1      # Learning rate\n",
    "gamma = 0.9      # Discount factor\n",
    "epsilon = 0.2    # Exploration rate\n",
    "episodes = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "def get_reward(state):\n",
    "    return 10 if state == (grid_size - 1, grid_size - 1) else -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(state):\n",
    "    x, y = state\n",
    "    return 0 <= x < grid_size and 0 <= y < grid_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = (0, 0)\n",
    "\n",
    "    while state != (grid_size - 1, grid_size - 1):\n",
    "        x, y = state\n",
    "\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_idx = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            action_idx = np.argmax(Q[x, y])\n",
    "\n",
    "        action = actions[action_idx]\n",
    "        dx, dy = action_dict[action]\n",
    "        next_state = (x + dx, y + dy)\n",
    "\n",
    "        if not is_valid(next_state):\n",
    "            next_state = state  # stay in place\n",
    "\n",
    "        reward = get_reward(next_state)\n",
    "        nx, ny = next_state\n",
    "\n",
    "        # Q-learning update\n",
    "        Q[x, y, action_idx] += alpha * (\n",
    "            reward + gamma * np.max(Q[nx, ny]) - Q[x, y, action_idx]\n",
    "        )\n",
    "\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,0): R\t(0,1): R\t(0,2): D\t(0,3): D\t\n",
      "(1,0): R\t(1,1): R\t(1,2): D\t(1,3): D\t\n",
      "(2,0): R\t(2,1): R\t(2,2): R\t(2,3): D\t\n",
      "(3,0): R\t(3,1): R\t(3,2): R\t(3,3): U\t\n"
     ]
    }
   ],
   "source": [
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        best_action = actions[np.argmax(Q[i, j])]\n",
    "        print(f\"({i},{j}): {best_action}\", end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
